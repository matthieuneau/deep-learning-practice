{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "# TP4 :  Learning on a low budget\n",
    "**Th√©o Rudkiewicz, Cyriaque Rousselot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "**Context :**\n",
    "\n",
    "Assume we are in a context where few \"gold\" labeled data samples are available for training, say \n",
    "\n",
    "$$\\mathcal{X}_{\\text{train}} = \\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$$\n",
    "\n",
    "where $N_{\\text{train}}$ is small. \n",
    "\n",
    "A large test set $\\mathcal{X}_{\\text{test}}$ exists but is not accessible. \n",
    "(To make your task easier, we provide you with some data (named `test_dataset` in the code) that you can use to test your model, but you **must not** use it to train your model).\n",
    "\n",
    "We also assume that we have a limited computational budget.\n",
    "\n",
    "The goal of this practical session is to guide you through different methods that will help you get better results from few resources (data & compute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "In this practical, we will use the `resnet18` architecture. We will use models from the [pytorch vision hub ](https://pytorch.org/vision/stable/models.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "# QUESTIONS\n",
    "\n",
    "## Grading\n",
    "\n",
    "You will be graded on 5 questions. You will need to provide 7 files : \n",
    "1. This Notebook\n",
    "2. `utils.py`\n",
    "3. `last_layer_finetune.pth` (the file **must be of size less than 5Mo**)\n",
    "4. `daug_resnet.pth` (the file **must be of size less than 50Mo**)\n",
    "5. `final_model.pth` (the file **must be of size less than 50Mo**)\n",
    "6. `drawing_lora.png`\n",
    "7. `cutmix.png`\n",
    "\n",
    "If the code you defined passes all our tests, you will get the full grade. Otherwise we  will look at the intermediate questions in the notebook to give you partial credit.\n",
    "\n",
    "\n",
    "\n",
    " Please provide clear and short answers between `<div class=\"alert alert-info\">  <your answer>  </div>` tags (when it's not code).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "<div class=\"alert alert-info\">  Example of answer  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "if not os.path.exists(\"data/TP4_images\"):\n",
    "    os.mkdir(\"data/TP4_images\")\n",
    "    !cd data/TP4_images && wget -O north_dataset_train.zip  \"https://nextcloud.lisn.upsaclay.fr/index.php/s/yzQRWE2YjmFn9WA/download/north_dataset_train.zip\" && unzip north_dataset_train.zip\n",
    "    !cd data/TP4_images && wget -O north_dataset_test.zip  \"https://nextcloud.lisn.upsaclay.fr/index.php/s/zntidWrFdYsGMDm/download/north_dataset_test.zip\" && unzip north_dataset_test.zip\n",
    "dir_path = \"data/TP4_images/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy, ConfusionMatrix\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "north_dataset = datasets.ImageFolder(\n",
    "    dir_path + \"north_dataset_sample\",\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    dir_path + \"north_dataset_test\",\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")\n",
    "base_model = models.resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "## Question 1 : \n",
    ">  Change the last layer of the resnet model so that its size fits the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hint \n",
    "base_model.fc = nn.Linear(512, 2)\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "assert (\n",
    "    base_model.fc.out_features == 2\n",
    ")  # we could also change the last layer to have 1 output. Do it with 2 so that it matches our tests procedure during grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "## Question 2: \n",
    "> Train the last layer of a randomly initialized resnet model. Provide a function precompute_features in `utils.py` that creates a new dataset from the features precomputed by the model.\n",
    "\n",
    "Intermediate question :  Provide the training process in the notebook with training curve. Comment on the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def precompute_features(\n",
    "    model: models.ResNet, \n",
    "    dataset: torch.utils.data.Dataset, \n",
    "    device: torch.device\n",
    ") -> torch.utils.data.Dataset:\n",
    "    \"\"\"\n",
    "    Create a new dataset with the features precomputed by the model.\n",
    "\n",
    "    If the model is $f \\circ g$ where $f$ is the last layer and $g$ is \n",
    "    the rest of the model, it is not necessary to recompute $g(x)$ at \n",
    "    each epoch as $g$ is fixed. Hence you can precompute $g(x)$ and \n",
    "    create a new dataset \n",
    "    $\\mathcal{X}_{\\text{train}}' = \\{(g(x_n),y_n)\\}_{n\\leq N_{\\text{train}}}$\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    model: models.ResNet\n",
    "        The model used to precompute the features\n",
    "    dataset: torch.utils.data.Dataset\n",
    "        The dataset to precompute the features from\n",
    "    device: torch.device\n",
    "        The device to use for the computation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.utils.data.Dataset\n",
    "        The new dataset with the features precomputed\n",
    "    \"\"\"\n",
    "    # dataset is small so we can use a single batch\n",
    "    dataloader = DataLoader(north_dataset, batch_size=len(north_dataset))\n",
    "\n",
    "    features_model = deepcopy(base_model)\n",
    "    features_model.fc = nn.Identity()\n",
    "\n",
    "    data = next(iter(dataloader))[0]\n",
    "    features = features_model(data)\n",
    "    targets = next(iter(dataloader))[1] \n",
    "\n",
    "    features_dataset = torch.utils.data.TensorDataset(features, targets)\n",
    "\n",
    "    return features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "from utils import precompute_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "<div class=\"alert alert-info\">  Example of answer  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "## Question 3 : \n",
    "> Now start from a pretained model on Imagenet (https://pytorch.org/vision/stable/models.html#) and only train the last layer. Provide the training process in the notebook with training curve. \n",
    "\n",
    " Provide two files : (https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    " -  a file  `utils.py` containing only the last layer class `LastLayer` inheriting from `torch.nn.Module` architecture of your final model to load\n",
    " -  a `last_layer_finetune.pth` file containing __only the last layer weights__ ( we will check the size) \n",
    " \n",
    " We will test your model on final accuracy on a test set. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "torch.save(base_model.fc.state_dict(), \"lastlayer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "from utils import LastLayer\n",
    "\n",
    "\n",
    "resnet = models.resnet18(weights=\"DEFAULT\")\n",
    "fc = LastLayer()  # !  Important : No argument\n",
    "fc.load_state_dict(torch.load(\"lastlayer.pth\", weights_only=True))\n",
    "resnet.fc = fc\n",
    "resnet.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "## Question 4 : \n",
    "> Perform  LoRA https://arxiv.org/pdf/2106.09685  on the model  (We are perfectly fine if you use an external library **for this question only**, and of course use it in the next questions). (Warning : without data augmentation it may not improve the accuracy.)\n",
    "\n",
    "Intermediate question : Describe LoRA. There are different ways of implementing LoRa for convolutions. You can choose your preferred one. Explain the version of LoRa you used, provide a drawing of the process in the `drawing_lora.png` file. (Hint: you can obtain a small rank convolution by combining a convolution and a 1x1 convolution. One of the two goes from a higher number of channels to a lower number of channels and the other one restores the number of channels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "torch.save(lora_model.state_dict(), \"lora_resnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "## Question 5 : \n",
    "In order to better train our LORA weights, let's do some Data Augmentation https://en.wikipedia.org/wiki/Data_augmentation . Load some alteration of the data from the `torchvision.transforms` module and incorporate them in your training pipeline.\n",
    "\n",
    " Intermediate question : Check CutMix  (https://pytorch.org/vision/stable/auto_examples/transforms/plot_cutmix_mixup.html#sphx-glr-auto-examples-transforms-plot-cutmix-mixup-py) and explain it with a small drawing `cutmix.png`. \n",
    "\n",
    "\n",
    "  Provide one file : (https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    " -  a `daug_resnet.pth` file containing the weight of the ResNet18 after DAUG  (  !  It  has to be of the class ResNet so you have to merge LoRA weights with the ResNet18 weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "## Data Augmentation\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "lora_model = NotImplementedError  # <YOUR CODE>\n",
    "assert isinstance(lora_model, models.ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "outputs": [],
   "source": [
    "torch.save(lora_model.state_dict(), \"daug_resnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": [
    "## Question 6 : (BONUS) \n",
    "> Do the best you can : improve performance on test set while keeping ResNet 18 architecture, or decrease the size of the model\n",
    "\n",
    "Provide a file  `final_model.pth` containing the weights of the final model and provide the class `FinalModel()` in the `utils.py` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "Qxep+GiJD8N7"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice-labs-lU_vlk8p-py3.12",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "Qxep+GiJD8N7",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
