{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "# TP 3  : Graph Neural Networks Architecture\n",
    "\n",
    "**ThÃ©o Rudkiewicz, Cyriaque Rousselot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "# TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "### Install Pytorch Geometric\n",
    "\n",
    "To handle graph data, we use the library Pytorch Geometric : https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "\n",
    "*   If you use _Google Colab_, simply run the following cell to install Pytorch Geometric (**advised**).\n",
    "*   If you plan using your _own environment_, follow the documentation to install Pytorch Geometric : https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html and skip the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "### Import required packages\n",
    "\n",
    "Run the following cell to import all required packages. This cell **must not** be modified.\n",
    "\n",
    "To significantly accelerate your training, it is advised to use GPU. Using Google Colab, you need to activate it : \n",
    "\n",
    "*   Edit --> Notebook Setting --> Hardware accelerator --> GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: dlopen(/Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch_scatter/_scatter_cpu.so, 0x0006): Symbol not found: __ZN2at4_ops16div__Tensor_mode4callERNS_6TensorERKS2_NSt3__18optionalIN3c1017basic_string_viewIcEEEE\n",
      "  Referenced from: <4A3195B8-9E71-3AE7-AE80-DBA66ADAC535> /Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch_scatter/_scatter_cpu.so\n",
      "  Expected in:     <DA215AD3-6EAE-3755-B6A5-A8EB4EF952B0> /Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: dlopen(/Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch_spline_conv/_basis_cpu.so, 0x0006): Symbol not found: __ZN5torch8autograd12VariableInfoC1ERKN2at6TensorE\n",
      "  Referenced from: <E5EBDDAB-B1B9-3199-B5D8-B1F2B3E48B6A> /Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch_spline_conv/_basis_cpu.so\n",
      "  Expected in:     <DA215AD3-6EAE-3755-B6A5-A8EB4EF952B0> /Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(\n",
      "/Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch_sparse/_spmm_cpu.so, 0x0006): Symbol not found: __ZN5torch8autograd12VariableInfoC1ERKN2at6TensorE\n",
      "  Referenced from: <240FBBBE-7919-300D-BFF9-4CBCFE26BD89> /Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch_sparse/_spmm_cpu.so\n",
      "  Expected in:     <DA215AD3-6EAE-3755-B6A5-A8EB4EF952B0> /Users/matthieuneau/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as graphnn\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We use the Protein-Protein Interaction (PPI) network dataset which includes:\n",
    "- 20 graphs for training \n",
    "- 2 graphs for validation\n",
    "- 2 graphs for testing\n",
    "\n",
    "One graph of the PPI dataset has on average 2372 nodes. Each node has:\n",
    "- 50 features : positional gene sets / motif gene / immunological signatures ...\n",
    "- 121 (binary) labels : gene ontology sets (way to classify gene products like proteins).\n",
    "\n",
    "**This problem aims to predict, for a given PPI graph, the correct nodes' labels**.\n",
    "\n",
    "**It is a node (multi-label) classification task** (trained using supervised learning, with labels to be predicted for each node). \n",
    "\n",
    "For your curiosity, more detailed information on the dataset and some applications:\n",
    "- https://cs.stanford.edu/~jure/pubs/pathways-psb18.pdf\n",
    "- https://arxiv.org/abs/1707.04638\n",
    "\n",
    "To understand how a graph data is implemented in Pytorch Geometric, refer to : https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the train dataset:  20\n",
      "Number of samples in the val dataset:  2\n",
      "Number of samples in the test dataset:  2\n",
      "Output of one sample from the train dataset:  Data(x=[1767, 50], edge_index=[2, 32318], y=[1767, 121])\n",
      "Edge_index :\n",
      "tensor([[   0,    0,    0,  ..., 1744, 1745, 1749],\n",
      "        [ 372, 1101,  766,  ..., 1745, 1744, 1739]])\n",
      "Number of features per node:  50\n",
      "Number of classes per node:  121\n",
      "tensor([[1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "### LOAD DATASETS\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Train Dataset\n",
    "train_dataset = PPI(root=\"\", split=\"train\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "# Val Dataset\n",
    "val_dataset = PPI(root=\"\", split=\"val\")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "# Test Dataset\n",
    "test_dataset = PPI(root=\"\", split=\"test\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Number of features and classes\n",
    "n_features, n_classes = train_dataset[0].x.shape[1], train_dataset[0].y.shape[1]\n",
    "\n",
    "print(\"Number of samples in the train dataset: \", len(train_dataset))\n",
    "print(\"Number of samples in the val dataset: \", len(test_dataset))\n",
    "print(\"Number of samples in the test dataset: \", len(test_dataset))\n",
    "print(\"Output of one sample from the train dataset: \", train_dataset[0])\n",
    "print(\"Edge_index :\")\n",
    "print(train_dataset[0].edge_index)\n",
    "print(\"Number of features per node: \", n_features)\n",
    "print(\"Number of classes per node: \", n_classes)\n",
    "print(train_dataset[0].y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "### Define a basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "Here we define a very simple Graph Neural Network model which will be used as our baseline. This model consists of three graph convolutional layers (from https://arxiv.org/pdf/1609.02907.pdf). The first two layers computes 256 features, followed by an ELU activation function. The last layer is used for (multi-label) classification task, computing 121 features (for each node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "################## MODEL ############################\n",
    "#####################################################\n",
    "class BasicGraphModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.graphconv1 = graphnn.GCNConv(input_size, hidden_size)\n",
    "        self.graphconv2 = graphnn.GCNConv(hidden_size, hidden_size)\n",
    "        self.graphconv3 = graphnn.GCNConv(hidden_size, output_size)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.graphconv1(x, edge_index)\n",
    "        x = self.elu(x)\n",
    "        x = self.graphconv2(x, edge_index)\n",
    "        x = self.elu(x)\n",
    "        x = self.graphconv3(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "Next function is designed to evaluate the performance of the model, computing the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "############### TEST FUNCTION #######################\n",
    "#####################################################\n",
    "def evaluate(model, loss_fcn, device, dataloader):\n",
    "    score_list_batch = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        output = model(batch.x, batch.edge_index)\n",
    "        loss_test = loss_fcn(output, batch.y)\n",
    "        predict = np.where(output.detach().cpu().numpy() >= 0, 1, 0)\n",
    "        score = f1_score(batch.y.cpu().numpy(), predict, average=\"micro\")\n",
    "        score_list_batch.append(score)\n",
    "\n",
    "    return np.array(score_list_batch).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "Next we construct the function to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "############## TRAIN FUNCTION #######################\n",
    "#####################################################\n",
    "def train(\n",
    "    model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader\n",
    "):\n",
    "    epoch_list = []\n",
    "    scores_list = []\n",
    "    loss_per_epoch = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for i, train_batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_device = train_batch.to(device)\n",
    "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
    "            loss = loss_fcn(logits, train_batch_device.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        loss_data = np.array(losses).mean()\n",
    "        loss_per_epoch.append(loss_data)\n",
    "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
    "            print(\"F1-Score: {:.4f}\".format(score))\n",
    "            scores_list.append(score)\n",
    "            epoch_list.append(epoch)\n",
    "\n",
    "    return epoch_list, scores_list, loss_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "Let's train this model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device:  cpu\n",
      "Epoch 00001 | Loss: 0.6387\n",
      "F1-Score: 0.4488\n",
      "Epoch 00002 | Loss: 0.5812\n",
      "Epoch 00003 | Loss: 0.5615\n",
      "Epoch 00004 | Loss: 0.5560\n",
      "Epoch 00005 | Loss: 0.5512\n",
      "Epoch 00006 | Loss: 0.5462\n",
      "Epoch 00007 | Loss: 0.5417\n",
      "Epoch 00008 | Loss: 0.5376\n",
      "Epoch 00009 | Loss: 0.5344\n",
      "Epoch 00010 | Loss: 0.5317\n",
      "Epoch 00011 | Loss: 0.5291\n",
      "F1-Score: 0.5287\n",
      "Epoch 00012 | Loss: 0.5264\n",
      "Epoch 00013 | Loss: 0.5240\n",
      "Epoch 00014 | Loss: 0.5216\n",
      "Epoch 00015 | Loss: 0.5196\n",
      "Epoch 00016 | Loss: 0.5176\n",
      "Epoch 00017 | Loss: 0.5155\n",
      "Epoch 00018 | Loss: 0.5134\n",
      "Epoch 00019 | Loss: 0.5110\n",
      "Epoch 00020 | Loss: 0.5083\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\\nDevice: \", device)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "basic_model = BasicGraphModel(\n",
    "    input_size=n_features, hidden_size=256, output_size=n_classes\n",
    ").to(device)\n",
    "loss_fcn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(basic_model.parameters(), lr=0.005)\n",
    "\n",
    "epoch_list, basic_model_scores, loss_per_epoch = train(\n",
    "    basic_model,\n",
    "    loss_fcn,\n",
    "    device,\n",
    "    optimizer,\n",
    "    max_epochs,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "Let's evaluate the performance of this basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Model : F1-Score on the validation set: 0.5229\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAHDCAYAAADss29MAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMv5JREFUeJzt3Qu4VGW9P/B3c0cTvJCgiKJmqeEtFCIr60hy1PTYSVMr4ZDWP4+ZSZaiiZkXUtPDSVHStLtHslN20ShFrUyKAi0rsUwLjidATgUKyXX+z2/BbGZmz97sjWz23i+fz/NMzqxZM7PWzNq0vut939/bUCqVSgkAACAj3Tp6AwAAALY0QQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BB9imNTQ0pE9+8pNb9D2/+MUvFu/7pz/9KXVm1113Xdpnn31S9+7d06GHHtrRmwONHn744eJv6Bvf+EZHbwrQhQk6QIcrB4Pmbj/72c9SZ3T11Vene+65J3VFP/zhD9PHP/7xdOSRR6YvfOELxb4059/+7d+a/W1mzJjRuN4tt9ySTjnllLTnnnsWz8Xr2uq73/1uOuqoo9Kuu+6atttuuyKIvetd76r6HPI7HgHaQ492eVeAzfCpT30q7b333k2Wv+pVr0qd9cTy5JNPTieddFLV8jPOOCOddtppqXfv3qmzevDBB1O3bt3S7bffnnr16rXJ9WNfPv/5zzdZfsghhzTev+aaa9ILL7yQRowYkf7yl7+0eZs+85nPpI997GNF0Jk4cWIRdJ5++un0wAMPpLvuuiv98z//c5vfc1vS3PEIsK0SdIBO49hjj02HH3546uqiK1jcOrPFixenvn37tirkhB49eqT3vve9La7zox/9qLE15xWveEWbtmfNmjXpiiuuSG9729uK1qZ627u1rFu3Lq1atSr16dMndUbLly9P22+/fUdvBkCnp+sa0CWsXr067bzzzmn8+PFNnlu2bFlxUnrBBRdUnRifeeaZaeDAgcVz0fLwpS99aZOfE92thg4d2mR5jOOJE/iyuB8nnPGe5W5c5a5azY3Rufnmm9NrX/vaonVk9913T+ecc076+9//XrXOW97yljRs2LD0u9/9Lr31rW8tWjUGDx6crr322jYFhn333bf4nNiXiy++OK1cubJq26O7Wmx/edtjm1+uvfbaq+o7aoslS5YUv2N0pasnurJVeumll4rf5NWvfnXx++62227pX//1X9Mf//jHxnVi/z760Y+mIUOGFN/Fa17zmqLVqFQqVb1XbPOHPvSh9LWvfa3x9yl3lXvuuefS+973vuI4iuXx/B133LHJ/Ylted3rXle17IQTTig+6zvf+U7jsp///OfFsu9///t13yeOqQiNsV/HHXdc2mGHHdJ73vOeJuu1dDw2J46Jyy67rGgxjX2L7ym6M1YeK7XfT3yH8X0PHz48/fjHP27yno899lhxwaJfv37Fdh999NF1u57GcX/++ecXx2d89h577JHGjh1bHAe1ofOqq64qno/PjfeLVr5Kf/jDH9I73/nONGjQoGKdWDdaVJcuXdri/gP506IDdBpxYlJ7ohMnWbvsskvq2bNnesc73pG++c1vps997nNVLRExLiFOzuLkJvzjH/8oAkOcEMUJWnSHu/vuu4sTvzjBOu+88172tn7lK19JZ511VtFN6wMf+ECxLMJFc+Kk/PLLL0+jR49OZ599dnrqqaeKMS2/+MUv0k9/+tNi/8r+9re/Fd204mQ5xqfEgOwLL7wwHXTQQcVJZEtim+JkN7owxUl+nEhPnjw5Pfnkk+lb3/pW47bfeuutafbs2Y3d0d7whjdscp9rf5vY5v79+6ctIYJMtDDFGJ1zzz23CLXNWbt2bXr729+eZs6cWfzm8XtGl7n7778//eY3vyl+hwgzJ554YnrooYeKwBvFFn7wgx8UXeMivPzHf/xHk658X//614vjZcCAAcUJ+KJFi9LrX//6xhP9V77ylUUgifeLUPaRj3yk2W1805velL797W8X68VJf2xP/M7RXfAnP/lJsW0h7sey5gJeObyOGTMmvfGNbyyCWoTfl3s8RoCIbXjkkUeK9Q844ID0xBNPFN/L73//+yZjfaK1bvr06enDH/5wEUwitMcxGsdQBPPw29/+ttjv2N8ITHF8xN9q/C3G60eOHFms9+KLLxbrxTEZITICYRxbEQD/53/+p/j+yz796U8X309cxIh/HyLwR9CL4zpEy1t8N/H3H8dNhJ34fb/3ve8Vf+tb6vgEuqgSQAf7whe+EJfY69569+7duN4PfvCDYtl3v/vdqtcfd9xxpX322afx8ZQpU4r1vvrVrzYuW7VqVWnUqFGlV7ziFaVly5Y1Lo/1LrvsssbH48aNK+21115NtjHWqf0nc/vtty/Wb25/nn322eLx4sWLS7169Sodc8wxpbVr1zaud9NNNxXr3XHHHY3LjjrqqGLZl7/85cZlK1euLA0aNKj0zne+s8Xv8fHHHy9ee9ZZZ1Utv+CCC4rlDz74YNV+xva3Rqxb77eJbW1Oc99NSyZNmlS8b7z22GOPLV111VWlOXPmNFkvvq9Y74Ybbmjy3Lp164r/3nPPPcU6V155ZdXzJ598cqmhoaH09NNPNy6L9bp161b67W9/W7XumWeeWdptt91KS5YsqVp+2mmnlfr3719asWJFs/vyi1/8onjf++67r3j861//unh8yimnlEaOHNm43oknnlg67LDDNvndX3TRRaVNact3/pWvfKXY55/85CdVy6dNm1Z83k9/+tPGZeXf+5e//GXjsj//+c+lPn36lN7xjnc0LjvppJOK4/yPf/xj47L//d//Le2www6lN7/5zU1+529+85vN/n4PPfRQsc4BBxxQHP9l//mf/1ksf+KJJ4rHjz32WPH47rvvbtV+A9sWXdeATmPq1KnFVfnKW2WXnn/6p38qrvbGleXK1o9Y79RTT21cdt999xVXdk8//fTGZXF1Oa5Gx9XkuLq8NcVg+rjyHC0AcXW67P3vf39x9fvee++tWj+6/FSOh4nWq7hS/8wzz7T4ObHfYcKECVXLo2Un1H5OW0SXoNrf5vrrr09bUrR43Xnnnemwww4rWl8uueSSootUXPGPq/9l//3f/10cB3EFv1a561x8FzFOKn7z2u8izt1ru4pFAYQDDzyw8XGsE58T3c3ifrQ4lG/RghCtC3Pnzm12X2If4ncsd++Klpty96x43YoVK4r3jRaVaN3YlGgF3JKihTNacfbff/+qfYu/sRAtYZVGjRpV/BZlMRbrX/7lX4rfKVrY4hZjq6IQQlTKK4suhe9+97uL/YzWrRDfa3QljRbaWrVdH6OramXrbfm7Kv8tlFtsYjviOwWopOsa0GnEyXxLxQhiQHz0xY+T4eiqEl1ooitbjN+pDDp//vOf03777VcVKkKc2JWf35rKnxfjGyrFCVycFNZuT5wQ157w7bTTTunXv/71Jj8n9rm2Sl2Evh133PFl7XeEhuh293JFt8LasROxfWURTuMWJ8XRPSnGDsXvHYEjuqVF4IrxKvFdxvHQnNjXGAcVY1pacwzUVvt7/vnni65P0cUvbvW0VCAhvq8IBxFwQvw3TtKj+1mEghi3EuN+/vrXv24y6MR+xjGxJcW4lgiP0R2vNfsWf0+1YnxUhIv4rkLcrz3Gy995dJVbsGBBMcYpfr/4O26NCFS1fwflCxzl3y2C/Q033FCMIYrvMrrkxYUC3dYAQQfoUmJMRvT7jyvycfU4xlXEVenKMscvR3OD6ePkdGtprmJb7SD65mxuQYCtIVrjagtK1NuvaOmKCmxxi9a4GHcUwSdaXtpDjA+qFCfmIU6Yx40bV/c1Bx98cIvvGaEmBtJH4YQIOtFCFYEzxrTE4wg6YVNBJwJ9bWh/uWL/YsxXBIR6ojBBZ9Cav4VoWYzxdzEmKlqVohUvxqVFmNzSARHoWgQdoEt585vfXHSHiRPmOJGMQeRxAllb/StaP+JkrvIEcd68eY3PNyeuGNdWQgv1WkNaGyjKnxcFCCq79UR3tmeffXaLtJSUPyf2Oa7Wl1suQgyqj31qab+3luj2Fd3e2iJa+SLolOfmiUH2EXqiJa+yiEOl2NfoMhhFCipbdVpzDIRo6YjXRcDd3N8nAkz8xv/1X/9VDJAvB5o4hstBJ1pFyoHn5WpLwI3v8Fe/+lVRxaw1r4tjqlYULYjCCOVWobgfx3it+M7j77AcnuKzo3VuS4rQFrdPfOIT6dFHHy2KO0ybNi1deeWVW/RzgK7FGB2gS4kTpqgoFtW5otJUVKSq7LYWogzvwoULq8byxHo33nhjMW6ipVaBOAmLrlWV3cTiBLtcsaxSzGVSLxTVihPl6Kb22c9+tupKdEzWGZ91/PHHpy0h9jtMmTKlann5qv2W+pyXI0JqfB+Vt3K3p1mzZtV9TXk8TblbVHR7ivEkN910U5N1y99vfBcRUmrXiapicWK/qep10ZIQnxPjSeqdlJe7a7UkqoxFEIuJVKOKXHTbChF4orUhxopVtubEcRahIALcpsR68+fP36zjMUQ1vwhft912W93uhVGqulL8NpVjkqIbWrSgHHPMMY3zRsX9WFZZVj1CdnQ9jIsS0UoX4nuNkFXvb6q1rZZl0cUx/rYrReCJfydqy2QD2x4tOkCnESe05SvulaL0cWVLSASbCC0xB0ic1FS2XoQolxvd26I7y5w5c4pSwVGiOcr7RgioHbdR2zUuSjnHQOnoAhMn4FEGOq681w4+j8HZ0WoQQSLGg8R4gXIJ3UpxxXvixInFYPsoyRtjCOLKd5ToPeKIIzY5EWdrRfe96GYVY0rihDcCXZT/jdaQ6OYX8/K0pwifcQIb4mQ9wmL5inrsc0tdveJ7jt85yjnHdxRX/2MfosxxtH7E9scA/xAD+r/85S8XYzNi/yIsxIl5/Bb//u//XgySjzE9sb/R2hcn3vHdRLemOBGPohAtlV6uLG0cg/LjN43CEVGsIMbUxHEQnxX3WxItHHGMRKgpz6FTbtGJ7Y1bZdCJYyR+q2jlqzeXU6U45uP3ffjhh9t8PIYzzjij6Pb5wQ9+sNjHaAGJYBh/f7E8BvdXjpeL7nbRGldZXjrEMV0Wv3W01kWoid8hxhbF32EEjsp5oKLEd/w9nnLKKUV56dju+C6jvHS0wrSlG2q06Ebp73iv+BuN0BMXQMpBFdjGdXTZN4CWykvHLZ6vLUE7ZMiQuuWDyxYtWlQaP358acCAAUXJ24MOOqjJ+9QrLx1++MMfloYNG1a87jWveU1Rprpeeel58+YVZXP79u1bPFcu7VtbXrqynPT+++9f6tmzZ2ngwIGls88+u/S3v/2tap0o2fza1762yXY2V/a61urVq0uXX355ae+99y4+J76niRMnll566aUm79eW8tKtWbe5MtT1fsN6233bbbcVJYpjP6Os+HbbbVeUXr7uuuuqSgyHKO18ySWXNO5nlN+O0tGVpY1feOGF0vnnn1/afffdi3X222+/4r3KJYzLYvvOOeecZo+jeC6+x/LnHH300aVbb7211Bof+9jHive/5pprqpa/6lWvKpZXbm/5+6s8bpr77uuV927ueGxOlFyP7YrjLb7vnXbaqTR8+PDi+Fm6dGnVZ8V3EH8H8R3GuvG7RAnoWnPnzi2NGTOmKOMev99b3/rW0qOPPtpkvf/7v/8rfehDHyoNHjy4+DvbY489iu0tl/Iul5euLRsd303l8fTMM8+U3ve+95X23Xffotz1zjvvXHzmAw880OK+A9uGhvifjg5bAEDnFC1R55xzTt2uggCdmTE6AABAdgQdAAAgO4IOAACQnTYHnR//+MdF9Zio6BL9dqMizqZEVZjXve51RaWWmLE7ZroGADq/GMprfA6wTQSdKIcZpR+nTp3aqvWjTGbM3RBlPh9//PGirOdZZ51VlK4EAABoDy+r6lq06MSEXzG/QXNiPop77723asK1mKci5keYMWPG5n40AABAx00YGrMpl2e+LotJx6JlpzkxuVjljMbr1q0rJhPbZZddGidcAwAAtj2lUim98MILxVCabt26dVzQWbhwYRo4cGDVsni8bNmy9I9//CP17du3yWsmT55cNdsyAABApQULFqQ99tgjdVjQ2RwTJ05MEyZMaHy8dOnStOeeexY7069fvw7dNgAAoONEg8mQIUPSDjvs0OJ67R50Bg0alBYtWlS1LB5HYKnXmhOiOlvcasVrBB0AAKBhE0Na2n0enVGjRqWZM2dWLbv//vuL5QAAAO2hzUHnxRdfLMpEx61cPjruz58/v7Hb2dixYxvX/+AHP5ieeeaZ9PGPfzzNmzcv3XzzzenrX/96Ov/887fkfgAAAGx+0PnlL3+ZDjvssOIWYixN3J80aVLx+C9/+Utj6Al77713UV46WnFi/p3rr78+ff7zny8qrwEAAHS6eXS25oCj/v37F0UJjNEBAIBt17JWZoN2H6MDAACwtQk6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7gg4AAJAdQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7gg4AAJAdQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7gg4AAJAdQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHY2K+hMnTo1DR06NPXp0yeNHDkyzZ49u8X1p0yZkl7zmtekvn37piFDhqTzzz8/vfTSS5u7zQAAAFs26EyfPj1NmDAhXXbZZWnu3LnpkEMOSWPGjEmLFy+uu/6dd96ZLrroomL9J598Mt1+++3Fe1x88cVt/WgAAID2CTo33HBDev/735/Gjx+fDjzwwDRt2rS03XbbpTvuuKPu+o8++mg68sgj07vf/e6iFeiYY45Jp59++iZbgQAAALZK0Fm1alWaM2dOGj169MY36NateDxr1qy6r3nDG95QvKYcbJ555pl03333peOOO67Zz1m5cmVatmxZ1Q0AAKC1erRl5SVLlqS1a9emgQMHVi2Px/Pmzav7mmjJide98Y1vTKVSKa1ZsyZ98IMfbLHr2uTJk9Pll1/elk0DAADYelXXHn744XT11Venm2++uRjT881vfjPde++96Yorrmj2NRMnTkxLly5tvC1YsKC9NxMAANhWW3QGDBiQunfvnhYtWlS1PB4PGjSo7msuvfTSdMYZZ6SzzjqreHzQQQel5cuXpw984APpkksuKbq+1erdu3dxAwAAaPcWnV69eqXhw4enmTNnNi5bt25d8XjUqFF1X7NixYomYSbCUoiubAAAAB3aohOitPS4cePS4YcfnkaMGFHMkRMtNFGFLYwdOzYNHjy4GGcTTjjhhKJS22GHHVbMufP0008XrTyxvBx4AAAAOjTonHrqqen5559PkyZNSgsXLkyHHnpomjFjRmOBgvnz51e14HziE59IDQ0NxX+fe+659MpXvrIIOVddddUW3REAAICyhlIX6D8W5aX79+9fFCbo169fR28OAADQybNBu1ddAwAA2NoEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7gg4AAJAdQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7gg4AAJAdQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7mxV0pk6dmoYOHZr69OmTRo4cmWbPnt3i+n//+9/TOeeck3bbbbfUu3fv9OpXvzrdd999m7vNAAAALeqR2mj69OlpwoQJadq0aUXImTJlShozZkx66qmn0q677tpk/VWrVqW3ve1txXPf+MY30uDBg9Of//zntOOOO7b1owEAAFqloVQqlVIbRLg54ogj0k033VQ8XrduXRoyZEg699xz00UXXdRk/QhE1113XZo3b17q2bNn2hzLli1L/fv3T0uXLk39+vXbrPcAAAC6vtZmgzZ1XYvWmTlz5qTRo0dvfINu3YrHs2bNqvua73znO2nUqFFF17WBAwemYcOGpauvvjqtXbu22c9ZuXJlsQOVNwAAgNZqU9BZsmRJEVAisFSKxwsXLqz7mmeeeaboshavi3E5l156abr++uvTlVde2eznTJ48uUhp5Vu0GAEAAHSaqmvRtS3G59x6661p+PDh6dRTT02XXHJJ0aWtORMnTiyaosq3BQsWtPdmAgAA22oxggEDBqTu3bunRYsWVS2Px4MGDar7mqi0FmNz4nVlBxxwQNECFF3hevXq1eQ1UZktbgAAAO3eohOhJFplZs6cWdViE49jHE49Rx55ZHr66aeL9cp+//vfFwGoXsgBAADY6l3XorT0bbfdlr70pS+lJ598Mp199tlp+fLlafz48cXzY8eOLbqelcXzf/3rX9N5551XBJx77723KEYQxQkAAAA6xTw6Mcbm+eefT5MmTSq6nx166KFpxowZjQUK5s+fX1RiK4tCAj/4wQ/S+eefnw4++OBiHp0IPRdeeOGW3RMAAIDNnUenI5hHBwAAaLd5dAAAALoCQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkp0dHbwAAANCxSqVSWrV2XfrHqrVpxYZb3F++ak3jsn1euX06YLd+qasQdAAAoItYtWZDGFm9ZmMYWbkmrVi9/v76ZeufW15xvzHArF6bVqzcsCzuVzy/Zl2pxc8++y37CjoAALCtWr12XUW42BgqIpBUBo4IIctXNg0c61tUmj6O9VavbTmMbAk9uzekvj27p+169Ujb9Y7/dk/b9eyRdt+xb+pKBB0AALY5ayKMrK4JEjWtIPWCx/rQsuH+yuqWlfJ/owtYe+vRrSH1jQDSq3vavlePxvt9I5xESOm9IaDEc0Vo2fh4/XrV9yvfo2f3PIbxCzoAAHRKa9eVGgNIZciIVpDKwFF+vnI8SWVrSmWQKbpuRRhZ0/5hpHu3hiJ0FEGi98bAUQ4U5aDRGFDqhZENLSvr32N9y0rc79UjjzDSngQdAAA227oIIxu6XlUGkrpdr2pbTBpbVGpbUNY/XrkVwki3hrQxSNQEjsaWkN4bWkkqni+3glQHl+qQ0qt7t9TQ0NDu+0B9gg4AwDYQRtaPA1lbdyB7+bnyQPb1YaT5cSOVA9lfWt3+YSSywvqWkcpWj40tHU1bRza2hJS7cPXtWf389hte27uHMJIrQQcAoJOU960KIzWtIJUtJuWQUQxkrxjY3lylrVi/vUVWqOyatX3luJENISO6XpXvN9cK0ni/5/ruXnFfGGFzCDoAAG0II9GCUVlJq7GloxiY3nQg+/JmxpiUW1Yqw0ip/QtqNRtGNjmQvar7VnULStzv01MYoXMRdACA7MJIjO2oGoBed9zIxrlH1pf4rT9OpLJL14qtFEYiNJRDRtEKUg4gbR3IvqEscDm49OnRPXWLQSmwDRB0AIAOm4W9pVaQ6pK968eNVE6GWG5NqRxjUg4om5j3cIuI7lR1x4aUW0k2dL0q36/XClK3+1ZPYQS2BEEHAGhWlOCtWxGrmF194ySGjS0fVTO0Vwx4rx03snptUTq4vUUJ3vWtGk1L/DZbaatei0nFQPbtNrxHlA4GOi9BBwBymIW9zsSF1QGlulpWa2doX7M1wkj3bhWTHdYZN9Kzx4buWxu6YTXbfas2nHRPPTKZ+BBoO0EHALbiLOzlVpCqgezRClJTOat+960NYWbDZInl1pTVa9s/jPTs3rChJWRDkKiYuLAyeGxfZ4b2xjEmlfOSlMv+CiNAOxF0AKBmFvYmM6nXndCwOnTUDmSvnaE9xqO0tx7dGpofqF5Zaauq+9YmBrJveNxTGAG6GEEHgC4XRoqWkHIrSE3IaDrBYZ1KW7Xdtza0rMR4lPYW4zo2DkyvHyyan6F9YytIvYHsMR4FgPUEHQDabRb2JvOHNCnZuzFolGdorxzIvnEQ+8bXRdng9hZjzCuDROXEhRvnIOlRhJHKFpF63bdq5yaJ8SjmGgFof4IOwDYcRl5as2H+kFWtH8jepMWkzkD2mFCxvUVWWN8yUtntqoWSvRsqbzW2mFTM0F47kN0s7ABdn6AD0IVmYa8dN1I1DqSiWlZrZmiP9beGqiBRMXFhbcjYvk4ryPruW9XzixQtJr17CCMAtEjQAdiCs7BXTly4yYHsVeV+1zQbRrbGLOwbu2M1P5C93ApSnq29shWkSSDZEFxidndhBICOIOgA21QYqQ0SVQPZy2V/V296IHtll64II1tjFvYIDZUho6UKWbWVtMpjTBrXqywLbBZ2ADIk6ACdKoxECd7aWdQrW0GqS/ZunHukXIGrcob2YtxIRUDZGmEkulM1N05k4xiR9TOrb6y81aOF7lsbW1DMwg4ArSfoAG0WJXjLg9cbB7JXtIRUd99qeSB7bYnfKB3c3qIE7/pWjaYzqVdV2toQPCpbTMoztNcbYxKvFUYAoHMQdCBTq2MW9jpjPpodN1IeyF6ecb2FGdrXbI0w0r1bkxnXq7plVYSMpt23mhnIviHcmIUdAPIn6EAHWhNhpHL+kIquV+WQUdmFq373rY1lgctjTOI9Vq8tbZVZ2GuDxvY1LSJtm6F942vNwg4AvByCDmxCdKWqnLhwRZ2B7Bu7b60PHc1X2qp+HONRtsos7JXdsmrGiGxqIHu97lvlgexmYQcAOitBh2zCSLnr1frWkYpKWnW7bzUzbqSxNWVjV60Yj7JVwkjjeJEWJkCsDCmNc4rUH19SDiZmYQcAtkWCDlt1FvZ6QaLc0lF+rnJgevm5yspZ9cr+Rtng9hZjzKsGqldMXLhxTpGWx400NzeJiQ8BALYsQYcmYeSlNc2FkYoWkzqlf6taTGpmZI/Xxezu7S2ywvrwsIkZ18sD2aN6Vs9NVdpaf18YAQDoOgSdLjrXSISG6okLm4aM9d23mpbzbTJuZPXGSlux/tZQVZK3YuLC6haPlgeyV3ffWr+eWdgBAAiCTjvPwl4bLCoDSb0Wk/K4ksoxJrUBJcJIaStMfLixO1Z1yIg5RCq7XTXtvtV03Ejl+/TpYRZ2AADal6DTBn9fsSrd+ODTdWdcr5qRfcNkiVtjFvZowaiqpFW3q1bFQPYNrSBVA9k3TH5Y2bISzwkjAAB0VYJOG0T1rdsfebbNr4uxHU1mXC9aRqorabU8kL3pDOzxHmZhBwCApgSdNujXt2f6f0ftU7R8rO++1fwM7ZWBRBgBAICtS9Bpgz49u6eJxx7Q0ZsBAABsgmnNAQCA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7gg4AAJCdzQo6U6dOTUOHDk19+vRJI0eOTLNnz27V6+66667U0NCQTjrppM35WAAAgPYJOtOnT08TJkxIl112WZo7d2465JBD0pgxY9LixYtbfN2f/vSndMEFF6Q3velNbf1IAACA9g06N9xwQ3r/+9+fxo8fnw488MA0bdq0tN1226U77rij2desXbs2vec970mXX3552meffdr6kQAAAO0XdFatWpXmzJmTRo8evfENunUrHs+aNavZ133qU59Ku+66azrzzDNb9TkrV65My5Ytq7oBAAC0S9BZsmRJ0TozcODAquXxeOHChXVf88gjj6Tbb7893Xbbba3+nMmTJ6f+/fs33oYMGdKWzQQAALZx7Vp17YUXXkhnnHFGEXIGDBjQ6tdNnDgxLV26tPG2YMGC9txMAAAgMz3asnKEle7du6dFixZVLY/HgwYNarL+H//4x6IIwQknnNC4bN26des/uEeP9NRTT6V99923yet69+5d3AAAANq9RadXr15p+PDhaebMmVXBJR6PGjWqyfr7779/euKJJ9Ljjz/eeDvxxBPTW9/61uK+LmkAAECHt+iEKC09bty4dPjhh6cRI0akKVOmpOXLlxdV2MLYsWPT4MGDi3E2Mc/OsGHDql6/4447Fv+tXQ4AANBhQefUU09Nzz//fJo0aVJRgODQQw9NM2bMaCxQMH/+/KISGwAAQEdpKJVKpdTJRXnpqL4WhQn69evX0ZsDAAB08myg6QUAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7gg4AAJAdQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7gg4AAJAdQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkZ7OCztSpU9PQoUNTnz590siRI9Ps2bObXfe2225Lb3rTm9JOO+1U3EaPHt3i+gAAAFs96EyfPj1NmDAhXXbZZWnu3LnpkEMOSWPGjEmLFy+uu/7DDz+cTj/99PTQQw+lWbNmpSFDhqRjjjkmPffccy974wEAAOppKJVKpdQG0YJzxBFHpJtuuql4vG7duiK8nHvuuemiiy7a5OvXrl1btOzE68eOHduqz1y2bFnq379/Wrp0aerXr19bNhcAAMhIa7NBm1p0Vq1alebMmVN0P2t8g27disfRWtMaK1asSKtXr04777xzs+usXLmy2IHKGwAAQGu1KegsWbKkaJEZOHBg1fJ4vHDhwla9x4UXXph23333qrBUa/LkyUVKK9+ixQgAAKBTVl379Kc/ne666670rW99qyhk0JyJEycWTVHl24IFC7bmZgIAAF1cj7asPGDAgNS9e/e0aNGiquXxeNCgQS2+9jOf+UwRdB544IF08MEHt7hu7969ixsAAEC7t+j06tUrDR8+PM2cObNxWRQjiMejRo1q9nXXXnttuuKKK9KMGTPS4YcfvlkbCgAA0C4tOiFKS48bN64ILCNGjEhTpkxJy5cvT+PHjy+ej0pqgwcPLsbZhGuuuSZNmjQp3XnnncXcO+WxPK94xSuKGwAAQIcHnVNPPTU9//zzRXiJ0HLooYcWLTXlAgXz588vKrGV3XLLLUW1tpNPPrnqfWIenk9+8pNbYh8AAABe3jw6HcE8OgAAQLvNowMAANAVCDoAAEB2BB0AACA7gg4AAJAdQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7gg4AAJAdQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAACyI+gAAADZEXQAAIDsCDoAAEB2BB0AACA7gg4AAJAdQQcAAMiOoAMAAGRH0AEAALIj6AAAANkRdAAAgOwIOgAAQHYEHQAAIDuCDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdjYr6EydOjUNHTo09enTJ40cOTLNnj27xfXvvvvutP/++xfrH3TQQem+++7b3O0FAADY8kFn+vTpacKECemyyy5Lc+fOTYccckgaM2ZMWrx4cd31H3300XT66aenM888Mz322GPppJNOKm6/+c1v2vrRAAAArdJQKpVKqQ2iBeeII45IN910U/F43bp1aciQIencc89NF110UZP1Tz311LR8+fL0ve99r3HZ61//+nTooYemadOmteozly1blvr375+WLl2a+vXr15bNBQAAMtLabNCjLW+6atWqNGfOnDRx4sTGZd26dUujR49Os2bNqvuaWB4tQJWiBeiee+5p9nNWrlxZ3MpiJ8o7BQAAbLuWbcgEm2qvaVPQWbJkSVq7dm0aOHBg1fJ4PG/evLqvWbhwYd31Y3lzJk+enC6//PImy6PlCAAA4IUXXihadrZI0NlaosWoshUousf99a9/TbvssktqaGjo8AQZgWvBggW60dEqjhnayjFDWzlmaCvHDF35eImWnAg5u+++e4vrtSnoDBgwIHXv3j0tWrSoank8HjRoUN3XxPK2rB969+5d3CrtuOOOqTOJH7kz/NB0HY4Z2soxQ1s5Zmgrxwxd9XhpqSVns6qu9erVKw0fPjzNnDmzqrUlHo8aNarua2J55frh/vvvb3Z9AACAl6vNXdeiS9m4cePS4YcfnkaMGJGmTJlSVFUbP3588fzYsWPT4MGDi3E24bzzzktHHXVUuv7669Pxxx+f7rrrrvTLX/4y3XrrrS974wEAALZI0Ily0c8//3yaNGlSUVAgykTPmDGjseDA/Pnzi0psZW94wxvSnXfemT7xiU+kiy++OO23335FxbVhw4alrii61MUcQrVd66A5jhnayjFDWzlmaCvHDNvC8dLmeXQAAAA6uzaN0QEAAOgKBB0AACA7gg4AAJAdQQcAAMiOoNMGU6dOTUOHDk19+vRJI0eOTLNnz+7oTaKTivLqRxxxRNphhx3Srrvumk466aT01FNPdfRm0YV8+tOfTg0NDekjH/lIR28Kndhzzz2X3vve96Zddtkl9e3bNx100EHFFA5Qz9q1a9Oll16a9t577+J42XfffdMVV1xRzDIP4cc//nE64YQT0u677178f1BUSq4Ux0pUXt5tt92KY2j06NHpD3/4Q+qsBJ1Wmj59ejGHUJTWmzt3bjrkkEPSmDFj0uLFizt60+iEfvSjH6Vzzjkn/exnPysmyF29enU65phjijmnYFN+8YtfpM997nPp4IMP7uhNoRP729/+lo488sjUs2fP9P3vfz/97ne/K+as22mnnTp60+ikrrnmmnTLLbekm266KT355JPF42uvvTbdeOONHb1pdBLLly8vznHj4n49cbx89rOfTdOmTUs///nP0/bbb1+cD7/00kupM1JeupWiBSeu0Mc/DmHdunVpyJAh6dxzz00XXXRRR28enVzMPRUtOxGA3vzmN3f05tCJvfjii+l1r3tduvnmm9OVV15ZzFUWEzNDrfj/np/+9KfpJz/5SUdvCl3E29/+9mLew9tvv71x2Tvf+c7iyvxXv/rVDt02Op+Ghob0rW99q+iVEiIyREvPRz/60XTBBRcUy5YuXVocU1/84hfTaaedljobLTqtsGrVqjRnzpyiea4sJkWNx7NmzerQbaNriH8Iws4779zRm0InFy2Bxx9/fNW/N1DPd77znXT44YenU045pbiQcthhh6XbbrutozeLTiwmcZ85c2b6/e9/Xzz+1a9+lR555JF07LHHdvSm0QU8++yzaeHChVX//9S/f/+iMaCzng/36OgN6AqWLFlS9GuNxFopHs+bN6/DtouuIVr/YpxFdDEZNmxYR28Ondhdd91VdI2NrmuwKc8880zRDSm6VV988cXFcfPhD3849erVK40bN66jN49O2gq4bNmytP/++6fu3bsX5zZXXXVVes973tPRm0YXsHDhwuK/9c6Hy891NoIObIUr9L/5zW+Kq2bQnAULFqTzzjuvGNMVBU+gNRdRokXn6quvLh5Hi078WxN95wUd6vn617+evva1r6U777wzvfa1r02PP/54cSEuuiM5ZsiRrmutMGDAgOLKx6JFi6qWx+NBgwZ12HbR+X3oQx9K3/ve99JDDz2U9thjj47eHDqx6B4bxU1ifE6PHj2KW4zpikGfcT+uvEKlqHp04IEHVi074IAD0vz58ztsm+jcPvaxjxWtOjGWIir0nXHGGen8888vKoXCppTPebvS+bCg0wrRDWD48OFFv9bKK2nxeNSoUR26bXROMWAvQk4M4nvwwQeLUp7QkqOPPjo98cQTxRXW8i2u1keXkrgfF1ugUnSHrS1bH2Mv9tprrw7bJjq3FStWFGOMK8W/LXFOA5sS5zIRaCrPh6MrZFRf66znw7qutVL0gY5m3TjxGDFiRFEFKUrwjR8/vqM3jU7aXS26Bnz7298u5tIp912NQXtR3QZqxXFSO4YrynbG/CjGdlFPXImPweXRde1d73pXMbfbrbfeWtygnpgfJcbk7LnnnkXXtcceeyzdcMMN6X3ve19HbxqdqPLn008/XVWAIC62RTGlOG6iq2NUBN1vv/2K4BPzMkXXx3Jltk4nykvTOjfeeGNpzz33LPXq1as0YsSI0s9+9rOO3iQ6qfjTqnf7whe+0NGbRhdy1FFHlc4777yO3gw6se9+97ulYcOGlXr37l3af//9S7feemtHbxKd2LJly4p/U+Jcpk+fPqV99tmndMkll5RWrlzZ0ZtGJ/HQQw/VPX8ZN25c8fy6detKl156aWngwIHFvztHH3106amnnip1VubRAQAAsmOMDgAAkB1BBwAAyI6gAwAAZEfQAQAAsiPoAAAA2RF0AACA7Ag6AABAdgQdAAAgO4IOAACQHUEHAADIjqADAABkR9ABAABSbv4/relKrV4xkU0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### F1-SCORE ON VALID DATASET\n",
    "score_valid = evaluate(basic_model, loss_fcn, device, val_dataloader)\n",
    "print(\"Basic Model : F1-Score on the validation set: {:.4f}\".format(score_valid))\n",
    "\n",
    "\n",
    "### PLOT EVOLUTION OF F1-SCORE W.R.T EPOCHS\n",
    "def plot_f1_score(epoch_list, scores):\n",
    "    plt.figure(figsize=[10, 5])\n",
    "    plt.plot(epoch_list, scores)\n",
    "    plt.title(\"Evolution of F1-Score w.r.t epochs\")\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_f1_score(epoch_list, basic_model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "# QUESTIONS\n",
    "\n",
    "## Grading\n",
    "\n",
    "You will be graded on 5 questions. You will need to provide at least 4 files : \n",
    "1. This Notebook\n",
    "2. `class_model_gnn.py`\n",
    "3. `model.pth` (the file **must be of size less than 50Mo** but 20Mo should be enough to get a very good model)\n",
    "4. `conv_as_message_passing.py`\n",
    "\n",
    "If the function you defined passes all the tests, you will get the full grade. Otherwise we  will look at the intermediate questions in the notebook to give you partial credit.\n",
    "\n",
    "\n",
    "\n",
    " Please provide clear, short and __bold font__ answers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "> Question 1 : Design, build and train a model with a F1-score higher than 93% on validation set (**HINT :** https://arxiv.org/pdf/1710.10903.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    " Provide two files : (https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    " -  a file  `class_model_gnn.py` containing the class inheriting from `torch.nn.Module` architecture of your final model to load\n",
    " -  a `model.pth` file : the model weights\n",
    " \n",
    " We will  test your model on final F1-Score on a test set. You must not use the test set for hyperparameter training.\n",
    " \n",
    "Intermediate question : \n",
    "\n",
    " Provide the script for training, and a plot of the training loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "StudentModel.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.005\u001b[39m)\n\u001b[32m      8\u001b[39m max_epochs = \u001b[32m200\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m _, _, loss_per_epoch = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fcn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fcn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m torch.save(model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mmodel.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m model.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33mmodel.pth\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader)\u001b[39m\n\u001b[32m     13\u001b[39m optimizer.zero_grad()\n\u001b[32m     14\u001b[39m train_batch_device = train_batch.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch_device\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch_device\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m loss = loss_fcn(logits, train_batch_device.y)\n\u001b[32m     17\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/practice-labs-lU_vlk8p-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: StudentModel.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from class_model_gnn import StudentModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = StudentModel()\n",
    "loss_fcn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "max_epochs = 200\n",
    "_, _, loss_per_epoch = train(\n",
    "    model,\n",
    "    loss_fcn=loss_fcn,\n",
    "    device=device,\n",
    "    optimizer=optimizer,\n",
    "    max_epochs=max_epochs,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "\n",
    "plt.plot(range(max_epochs), loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "## Conv 2D as Message Passing Neural Network\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The use of graph is a way to structure data by adding neighborhood information between features. This then allows to do operations on the data that are local to each node and its neighbors. This is the main idea behind Graph Neural Networks (GNNs). [`pytorch-geometric`](https://pytorch-geometric.readthedocs.io/en/latest/) is a library compatible with PyTorch that allows to easily implement GNNs. The most general structure is the [`MessagePassing`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html#torch_geometric.nn.conv.MessagePassing) class that is then used as a base for more specific GNNs as seen in the course ([Graph Convolutional Networks](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv) or [Graph AttenTion Convolution](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html#torch_geometric.nn.conv.GATConv)).\n",
    "\n",
    "On the other hand, you already know an operation that uses the structure of the data to do local operations: the convolution (https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). One can see the convolution as a specific case of the message passing neural network. The goal of this notebook is to show how to use the `MessagePassing` class to implement a convolutional neural network.\n",
    "You will be asked to implement 3 functions. You should give back those three functions in a file named `conv_as_message_passing.py`. These functions will then be automatically tested. So be sure to respect the function signature and the function name.\n",
    "\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "To make the implementation easier we will make some assumptions:\n",
    "- the input is a single image (batch size of 1) of size 'C x H x W'\n",
    "- the convolution will be a 3x3 kernel with stride 1 and padding 1.\n",
    "\n",
    "You may also assume that the Conv2D layer has no bias but it will be slightly penalized in the grading.\n",
    "\n",
    "Bonus points will be given if you can handle the cases that are not covered by those assumptions.\n",
    "\n",
    "\n",
    "## Questions\n",
    "\n",
    "### Question 2\n",
    "\n",
    "> Using the formalism used in the [`MessagePassing`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html#torch_geometric.nn.conv.MessagePassing) documentation (and on [wikipedia](https://en.wikipedia.org/wiki/Graph_neural_network#Message_passing_layers) with sligthly different notations), explain how theorically you can simulate a 2D convolution using the `MessagePassing` formalism. This may include a pre-processing step to transform the image into a graph and then a post-processing step to transform the graph back into an image. (:warning: Those steps should be independent of the parameters of the convolution, but not necessarily from the hyper-parameters.)\n",
    "$$\\mathbf{x}_{i}^{\\prime} = \\gamma_{\\mathbf{\\Theta}}\\left( \\mathbf{x}_{i},\\bigoplus\\limits_{j \\in \\mathcal{N}(i)}\\,\\phi_{\\mathbf{\\Theta}}\\left( \\mathbf{x}_{i},\\mathbf{x}_{j},\\mathbf{e}_{j,i} \\right) \\right),$$\n",
    "\n",
    "\n",
    "HINT : It is possible to do it with the following $\\gamma$ : \n",
    "\n",
    "$$ \\gamma_\\Theta : x,y \\mapsto y $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to consider an image as a graph where each pixel is a node and its neighbors are all the pixels covered by the convolution kernel. More specifically, for in image of size C x H x W and kernels of size 3x3, the neighborhood of $x_{ij}$ consists in the 9 pixels square centered at $x_{ij}$ for all the filters in the image (including $x_{ij}$ itself). Therefore a node has a total of 3x3 neighbors, except if it's located on the border of the image where in that case it has fewer neighbors\n",
    "\n",
    "The nodes are therefore of dimension C\n",
    "\n",
    "Then we can use the suggested function $$ \\gamma_\\Theta : x,y \\mapsto y + b_\\theta $$\n",
    "\n",
    "where $b_\\theta$ represents the bias of the layer\n",
    "\n",
    "The aggragation function to use is the **sum**\n",
    "\n",
    "We define $\\phi_\\Theta$ as follows:\n",
    " $$\\phi_\\Theta : x_i, x_j, e_{j,i} \\mapsto e_{j,i} \\times x_j$$\n",
    "\n",
    "where $e_{j,i}$ is a matrix of size $C_{out} \\times C_{in}$ containing the weights of the kernel relating to the node j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from conv_as_message_passing import image_to_graph, graph_to_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "> Implement the pre-processing function, you can use the follwing code skeleton (you may change the output type, it is just a strong suggestion):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "> Implement the post-processing function, you can use the follwing code skeleton:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "#### Recommended test cases\n",
    "\n",
    "We **encourage** you to test that you have the property that the pre-processing function followed by the post-processing function is the identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "outputs": [],
   "source": [
    "ref_conv = torch.nn.Conv2d(5, 7, kernel_size=3, padding=1, stride=1)\n",
    "image = torch.randn(5, 10, 11)\n",
    "g_image = image_to_graph(image, ref_conv)\n",
    "reconstructed_image = graph_to_image(g_image.x, 10, 11, ref_conv)\n",
    "assert torch.allclose(image, reconstructed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "> Implement the `Conv2dMessagePassing` class that will simulate a 2D convolution using the `MessagePassing` formalism. \n",
    "You should inherit from the `MessagePassing` class and only change the `__init__` and `message` functions (the `forward` function has already been changed for you). You should use the following code skeleton:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "source": [
    "## Test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[14., 24., 30., 22.],\n",
       "         [33., 54., 63., 45.],\n",
       "         [57., 90., 99., 69.],\n",
       "         [46., 72., 78., 54.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_conv = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=1, bias=False)\n",
    "t_conv.weight.data.fill_(1.0)\n",
    "t_tensor = torch.arange(1, 17).view(1, 4, 4).float()\n",
    "t_conv(t_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_ktag": "CeIiDvQzU6lQ"
   },
   "outputs": [],
   "source": [
    "from conv_as_message_passing import image_to_graph, graph_to_image, Conv2dMessagePassing\n",
    "\n",
    "c = 5\n",
    "h = 10\n",
    "w = 11\n",
    "\n",
    "ref_conv = torch.nn.Conv2d(c, 2, kernel_size=3, padding=1, stride=1, bias=False)\n",
    "image = torch.randn(c, h, w)\n",
    "g_image = image_to_graph(image, ref_conv)\n",
    "ref_conv.weight.data = torch.randn_like(ref_conv.weight.data)\n",
    "\n",
    "conv_mp = Conv2dMessagePassing(ref_conv)\n",
    "g_image = conv_mp(g_image)\n",
    "\n",
    "y_th = ref_conv(image)\n",
    "\n",
    "ref_conv.weight.data = torch.randn_like(ref_conv.weight.data)\n",
    "reconstructed_image = graph_to_image(g_image, h, w, ref_conv)\n",
    "\n",
    "y_th - reconstructed_image\n",
    "\n",
    "assert torch.allclose(y_th, reconstructed_image, atol=1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice-labs-lU_vlk8p-py3.12",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "CeIiDvQzU6lQ",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
